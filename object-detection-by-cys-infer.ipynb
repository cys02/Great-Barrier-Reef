{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Tensorflow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef)\n> Detect crown-of-thorns starfish in underwater image data","metadata":{}},{"cell_type":"markdown","source":"## Notebooks:\n- Train: [object-detection-by-cys-train](https://www.kaggle.com/code/cuiyushuai/object-detection-by-cys-train)\n\n- Infer: [object-detection-by-cys-infer](https://www.kaggle.com/code/cuiyushuai/object-detection-by-cys-infer)","metadata":{}},{"cell_type":"markdown","source":"<br><br>\n# Install Libraries\n> 安装库\n\n- -q选项用于将安装过程设为静默模式\n- -U选项用于确保安装最新版本的包\n- Loguru 是一个 Python 日志库，它提供了简单且强大的日志记录功能，使开发者能够更轻松地管理和输出日志信息。\n- bbox-utility 是一个用于边界框操作的 Python 库\n    - coco.py - 主要用于将不同格式的标注数据转换为 COCO 数据集格式的 JSON 文件\n    - utils.py - 提供了一组用于处理图像边界框和标注数据的函数和类，适用于不同的边界框表示格式和计算机视觉任务。","metadata":{}},{"cell_type":"code","source":"# bbox-utility, check https://github.com/cys02/bbox for source code\n!pip install -q /kaggle/input/loguru-lib-dataset/loguru-0.5.3-py3-none-any.whl\n!pip install -q /kaggle/input/bbox-lib-dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:26:28.590136Z","iopub.execute_input":"2023-08-09T09:26:28.590477Z","iopub.status.idle":"2023-08-09T09:27:28.033885Z","shell.execute_reply.started":"2023-08-09T09:26:28.590443Z","shell.execute_reply":"2023-08-09T09:27:28.032880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><br>\n# Import Libraries\n> 导入库\n\n- `numpy`: 用于数值计算的基础库\n\n- `tqdm`: 提供可视化的进度跟踪（在循环中显示进度条）\n\n- `pandas`: 用于数据处理和分析的强大工具\n\n- `os`: 提供访问操作系统功能的接口\n\n- `cv2`: 导入 `OpenCV` 库，它是一个用于CV任务的开源库，提供图像和视频处理功能\n\n- `matplotlib`: 用于绘图和数据可视化的库\n\n- `glob`: 提供了查找文件路径的功能，可以根据通配符匹配模式获取文件列表\n\n- `shutil`: 提供了一些高级的文件和目录操作函数，比如复制、移动和删除等\n\n- `sys.path.append`: 将路径添加到Python解释器的搜索路径中，为了导入指定路径下的库\n\n- `torch`: 导入 `torch` 框架\n\n- `PIL`: 用于图像处理的Python库，提供了打开、操作和保存各种图像格式等功能","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:28.036210Z","iopub.execute_input":"2023-08-09T09:27:28.036464Z","iopub.status.idle":"2023-08-09T09:27:28.045284Z","shell.execute_reply.started":"2023-08-09T09:27:28.036434Z","shell.execute_reply":"2023-08-09T09:27:28.044554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><br>\n# Key-Points\n> 关键点\n\n* 与以往的目标检测比赛有所不同，参赛者必须使用**python时序API**提交预测\n* 每行预测都需要包含图像的所有边界框。提交格式采用**COCO**格式，即 `[x_min、y_min、width、height]`\n* 比赛的评价指标采用F2指标，它容忍一些假正例（FP），以确保较少海星被错过。这意味着解决假反例（FN）比假正例（FP）更重要\n$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$","metadata":{}},{"cell_type":"markdown","source":"<br><br>\n# Meta Data\n> 元数据，对数据集中元素的介绍\n\n* `train_images/` - 包含训练集照片的文件夹，照片的命名形式为 `video_{video_id}/{video_frame}.jpg`.\n* `[train/test].csv` - 图像的元数据。与其他测试文件一样，在提交笔记本之前，大部分测试元数据只能在笔记本中使用，只有前几行可供下载\n* `video_id` - 图像所属视频的ID编号。数据集中的图片来源于三个视频的抽帧，这个参数表明图片来自于哪个视频\n* `video_frame` - 视频中图像的帧号。从潜水员浮出水面开始，帧号中偶尔会出现空缺\n* `sequence` - 给出视频中无间隙子集的 ID。序列 ID 并非有序排列\n* `sequence_frame` - 给出序列中的帧号。\n* `image_id` - 图像的 ID 代码，格式为 `{video_id}-{video_frame}`\n* `annotations` - 检测到的海星的的边界框，可直接用 Python 评估的字符串格式表示。与将、提交的预测使用的格式不同。在 test.csv 中不可用。边界框由图像左下角的像素坐标\"(x_min, y_min) \"及其像素 \"宽 \"和 \"高 \"来描述（COCO 格式）。\n \n \n 在中文语境下，“annotations”直译为“注释”，所以后文中可能用“**注释**”、“**边界框**”或“**标注框**”来表示图片中**对海星的选择框**\n \n *  `CKPT_PATH` - 指定模型检查点的路径，导入训练好的模型 `best.pt` \n *  `CONF` - 置信度阈值（Confidence Threshold）。在目标检测任务中，模型会预测每个检测框中物体存在的置信度。只有当置信度高于这个阈值时，检测结果才会被保留。\n *  `IOU` - 交并比阈值。IOU用于判断两个检测框的重叠程度，如果两个框的IOU大于这个阈值，它们可能会被认为是同一个目标。","metadata":{}},{"cell_type":"code","source":"# 指定根目录路径\nROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\n# 指定模型的检查点路径（训练好的模型best.pt）\nCKPT_PATH = '/kaggle/input/reef-baseline-fold12/l6_3600_uflip_vm5_f12_up/f1/best.pt'\n# 定义图像的大小\nIMG_SIZE  = 9000\n# 定义置信度阈值\nCONF      = 0.25\n# 定义了IOU（交并比）阈值\nIOU       = 0.40\n# 定义了是否进行数据增强\nAUGMENT   = True","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:28.046658Z","iopub.execute_input":"2023-08-09T09:27:28.046930Z","iopub.status.idle":"2023-08-09T09:27:28.055700Z","shell.execute_reply.started":"2023-08-09T09:27:28.046894Z","shell.execute_reply":"2023-08-09T09:27:28.054962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 读取训练数据\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\n# 在数据df中添加image_path列，列值为图片路径\ndf['image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\n# 在数据df中添加annotations列，列值为标注框坐标数据\ndf['annotations'] = df['annotations'].progress_apply(eval)\ndisplay(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:28.058931Z","iopub.execute_input":"2023-08-09T09:27:28.059146Z","iopub.status.idle":"2023-08-09T09:27:28.508068Z","shell.execute_reply.started":"2023-08-09T09:27:28.059119Z","shell.execute_reply":"2023-08-09T09:27:28.507149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of BBoxes\n> 查看多少图片有标注框\n\n通过前面对数据集的查看，我们发现前两个数据的 `annotations` 都为空，也就是没有标注框，下面我们查看一下有多少数据有标注","metadata":{}},{"cell_type":"code","source":"# 使用progress_apply函数将lambda函数应用于df['annotations']列的每个元素\ndf['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\n# 计算有多少图像没有边界框，有多少图像有边界框，并将数值转换为百分比\ndata = (df.num_bbox>0).value_counts(normalize=True)*100\n# 输出\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:28.510626Z","iopub.execute_input":"2023-08-09T09:27:28.510995Z","iopub.status.idle":"2023-08-09T09:27:28.628104Z","shell.execute_reply.started":"2023-08-09T09:27:28.510955Z","shell.execute_reply":"2023-08-09T09:27:28.627351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><br>\n# Helper\n> 辅助工具\n\n* `coco2yolo`, `coco2voc`, `voc2yolo` - 用于在不同的标注格式之间进行转换\n* `draw_bboxes` - 用于在图像上绘制边界框\n* `load_image` - 用于加载图像\n* `clip_bbox` - 用于裁剪边界框，使其适应图像的边界\n* `str2annot` - 用于将字符串格式的边界框注释转换为字典格式\n* `annot2str` - 用于将字典格式的边界框注释转换为字符串格式","metadata":{}},{"cell_type":"code","source":"from bbox.utils import coco2yolo, coco2voc, voc2yolo, voc2coco\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\n# 参数annots表示边界框的注释列表\ndef get_bbox(annots):\n    # 注释字典的值转换为列表，并将结果存储在bboxes变量中\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\n# 参数row表示数据集中的一行数据\ndef get_imgsize(row):\n    # 函数用imagesize.get函数获取该图像的宽度和高度，宽度和高度被添加到row字典中\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    # 返回更新后的row字典\n    return row\n\n# 设定随机数生成器的种子为32，以确保每次运行代码时产生相同的随机数序列\nnp.random.seed(32)\n\n# 这一行代码使用列表推导式生成一个包含一个随机颜色元组的列表\n# 列表中只有一个元素，因为循环的范围是range(1)，这样就只生成了一个随机颜色\n# 这些随机颜色可以用于在图像上绘制边界框时进行标记或区分不同的边界框\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:28.629507Z","iopub.execute_input":"2023-08-09T09:27:28.629960Z","iopub.status.idle":"2023-08-09T09:27:28.639472Z","shell.execute_reply.started":"2023-08-09T09:27:28.629921Z","shell.execute_reply":"2023-08-09T09:27:28.638591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><br>\n# [YOLOv5](https://github.com/ultralytics/yolov5/)\n> 导入YOLOv5\n\n- 在第一段代码中，我们创建了相应的配置文件夹，并将字体数据复制到文件夹下\n- 在第二段代码中，我们定义了 `load_model` 函数，用于加载YOLOv5模型","metadata":{}},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:28.641361Z","iopub.execute_input":"2023-08-09T09:27:28.641857Z","iopub.status.idle":"2023-08-09T09:27:30.609657Z","shell.execute_reply.started":"2023-08-09T09:27:28.641816Z","shell.execute_reply":"2023-08-09T09:27:30.608552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load_model函数，用于加载YOLOv5模型\n# 参数：模型的路径，置信度阈值，IoU 阈值\ndef load_model(ckpt_path, conf=0.25, iou=0.50):\n    # 从输出数据中加载yolov5模型\n    # custom 表示加载一个自定义的模型\n    # path=ckpt_path 指定了检查点的路径（训练好的模型）\n    # source='local' 表示从本地加载模型\n    # force_reload=True 强制重新加载模型\n    model = torch.hub.load('/kaggle/input/yolov5-lib-dataset',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)\n    # 置信度阈值\n    model.conf = conf\n    # 交并比阈值\n    model.iou  = iou\n    # 不对特定类别进行过滤，即所有类别都会被考虑\n    model.classes = None\n    # 表示每个框只能对应一个标签\n    model.multi_label = False\n    # 表示每张图像的最大检测数为 1000\n    model.max_det = 1000\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:30.611711Z","iopub.execute_input":"2023-08-09T09:27:30.612075Z","iopub.status.idle":"2023-08-09T09:27:30.619436Z","shell.execute_reply.started":"2023-08-09T09:27:30.612043Z","shell.execute_reply":"2023-08-09T09:27:30.618710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><br>\n# Inference\n> 推理","metadata":{}},{"cell_type":"markdown","source":"## Helper\n> 辅助推理的方法\n\n-  `predict` - 使用给定的模型对输入图像进行预测，并返回检测到的边界框和对应的置信度\n-  `format_prediction` - 用于将置信度和边界框格式化为一串字符串注释\n-  `show_img` - 用于在图像上绘制边界框并返回可视化结果","metadata":{}},{"cell_type":"code","source":"# predict函数使用给定的模型对输入图像进行预测，并返回检测到的边界框和对应的置信度\n# model: 预测使用的目标检测模型\n# img: 输入图像\n# size: 自定义推理大小，默认为768\n# augment: 是否进行数据增强，默认为False\ndef predict(model, img, size=768, augment=False):\n    # 获取高度和宽度\n    height, width = img.shape[:2]\n    # 使用模型对输入图像进行预测，不进行数据增强\n    results = model(img, size=size, augment=augment)\n    # 将预测结果转换为 pandas 数据帧，并提取其中的边界框坐标信息\n    preds   = results.pandas().xyxy[0]\n    # 从预测结果中提取边界框的 xmin、ymin、xmax 和 ymax 坐标，保存在一个 numpy 数组中\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    # 如果存在边界框结果\n    if len(bboxes):\n        # 将边界框坐标从 VOC 格式转换为 COCO 格式，并将结果转换为整数类型\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        # 提取预测结果中的置信度值\n        confs   = preds.confidence.values\n        # 返回边界框和置信度\n        return bboxes, confs\n    else:\n        return [],[]\n\n# format_prediction函数用于将置信度和边界框格式化为一串字符串注释\n# 参数包含边界框的位置和置信度\ndef format_prediction(bboxes, confs):\n    # 初始化注释字符串\n    annot = ''\n    # 如果存在边界框\n    if len(bboxes)>0:\n        # 使用循环遍历每个边界框\n        for idx in range(len(bboxes)):\n            # 从边界框坐标中提取 xmin、ymin、宽度 w 和高度 h\n            xmin, ymin, w, h = bboxes[idx]\n            # 提取对应的置信度\n            conf             = confs[idx]\n            # 将置信度和边界框坐标格式化为字符串，按一定格式连接起来\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            # 在每个边界框的字符串之间添加空格\n            annot +=' '\n        # 去除字符串末尾的多余空格\n        annot = annot.strip(' ')\n    # 返回格式化后的字符串注释\n    return annot\n\n# show_img用于在图像上绘制边界框并返回可视化结果\ndef show_img(img, bboxes, bbox_format='yolo'):\n    # 创建一个列表 names，其中包含 'starfish' 字符串，长度与边界框数目相同\n    names  = ['starfish']*len(bboxes)\n    # 创建一个列表 labels，其中包含全为 0 的整数，长度与边界框数目相同\n    labels = [0]*len(bboxes)\n    # 使用外部的 draw_bboxes 函数，在图像上绘制边界框\n    img    = draw_bboxes(img = img,                     # 输入的图像，表示要在其上绘制边界框\n                           bboxes = bboxes,             # 边界框坐标的列表或数组，表示要绘制的边界框\n                           classes = names,             # 类别名称的列表，表示每个边界框对应的类别名称\n                           class_ids = labels,          # 类别标签的列表，表示每个边界框对应的类别标签\n                           class_name = True,           # 一个布尔值，表示是否在边界框上显示类别名称\n                           colors = colors,             # 边界框的颜色，表示每个类别对应的颜色\n                           bbox_format = bbox_format,   # 边界框坐标格式，这里是'yolo'\n                           line_thickness = 2)          # 绘制边界框时的线条粗细\n    # 将绘制后的图像转换为 PIL 图像，并调整大小为 (800, 400)，最终返回可视化的 PIL 图像\n    return Image.fromarray(img).resize((800, 400))","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:30.621277Z","iopub.execute_input":"2023-08-09T09:27:30.621548Z","iopub.status.idle":"2023-08-09T09:27:30.637314Z","shell.execute_reply.started":"2023-08-09T09:27:30.621512Z","shell.execute_reply":"2023-08-09T09:27:30.636479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Train**\n> 在训练集上运行推理，可视化推理结果","metadata":{}},{"cell_type":"code","source":"# 调用函数加载模型\nmodel = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n# 从 DataFrame df 中选择具有大于1个边界框的图像路径，随机选择其中100个，并将它们存储在名为 image_paths 的列表中。\nimage_paths = df[df.num_bbox>1].sample(100).image_path.tolist()\n# 遍历 image_paths 列表中的图像路径，使用索引 idx 和路径 path。\nfor idx, path in enumerate(image_paths):\n    # 使用 OpenCV 加载图像，然后通过切片操作将通道从 BGR 转换为 RGB\n    img = cv2.imread(path)[...,::-1]\n    # 使用预加载的模型 model 对当前图像 img 进行目标检测，返回检测到的边界框坐标和置信度\n    bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    # 使用 show_img 函数显示带有绘制的边界框的图像。\n    # 图像、边界框坐标和边界框坐标格式（'coco'）被传递给 show_img 函数，然后使用 display 函数显示图像。\n    display(show_img(img, bboxes, bbox_format='coco'))\n    # 仅显示部分图像\n    if idx>5:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:30.638793Z","iopub.execute_input":"2023-08-09T09:27:30.639102Z","iopub.status.idle":"2023-08-09T09:27:40.270751Z","shell.execute_reply.started":"2023-08-09T09:27:30.639040Z","shell.execute_reply":"2023-08-09T09:27:40.269827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Init Environment\n> 初始化环境，比赛要求必须使用提供的 python 时序 API 提交运行结果","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\n# 初始化环境\nenv = greatbarrierreef.make_env()   # initialize the environment\n# 迭代器，它循环遍历测试集和提交的样本\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:40.272426Z","iopub.execute_input":"2023-08-09T09:27:40.272709Z","iopub.status.idle":"2023-08-09T09:27:40.308336Z","shell.execute_reply.started":"2023-08-09T09:27:40.272674Z","shell.execute_reply":"2023-08-09T09:27:40.307587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Test**\n> 将测试集带入模型进行预测","metadata":{}},{"cell_type":"code","source":"# 加载模型\nmodel = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n# 遍历迭代器 iter_test 中的测试图像和相关数据\n# img, pred_df 是从迭代器中解包得到的当前测试图像和相关数据\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    # 使用预加载的模型 model 对当前图像 img 进行目标检测，返回检测到的边界框坐标和置信度\n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    # 使用 format_prediction 函数将边界框坐标和置信度格式化为一串字符串的注释\n    annot          = format_prediction(bboxes, confs)\n    # 将格式化的注释赋值给测试数据中的 annotations 列\n    pred_df['annotations'] = annot\n    # 调用 env 的 predict 方法，将包含注释的预测数据传递给它。\n    env.predict(pred_df)\n    # 输出前三张图像的检测结果\n    if idx<3:\n        display(show_img(img, bboxes, bbox_format='coco'))","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:40.309822Z","iopub.execute_input":"2023-08-09T09:27:40.310098Z","iopub.status.idle":"2023-08-09T09:27:44.550962Z","shell.execute_reply.started":"2023-08-09T09:27:40.310061Z","shell.execute_reply":"2023-08-09T09:27:44.550104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><br>\n# Check Submission\n> 查看提交数据","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:27:44.552428Z","iopub.execute_input":"2023-08-09T09:27:44.553170Z","iopub.status.idle":"2023-08-09T09:27:44.579662Z","shell.execute_reply.started":"2023-08-09T09:27:44.553128Z","shell.execute_reply":"2023-08-09T09:27:44.577587Z"},"trusted":true},"execution_count":null,"outputs":[]}]}